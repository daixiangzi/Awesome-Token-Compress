# Awesome-Token-Compress
ðŸ”¥ðŸ”¥ðŸ”¥ A paper list of some recent  works about Token Compress for Vit and VLM.
## VLM
### 2024

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2412.13303-red?logo=arxiv" height="14" />  [FastVLM: Efficient Vision Encoding for Vision Language Models](https://arxiv.org/pdf/2412.13303). [FastVLM;Apple;]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2412.09613-red?logo=arxiv" height="14" />  [PVC: Progressive Visual Token Compression for Unified Image and Video Processing in Large Vision-Language Models](https://arxiv.org/pdf/2412.09613). [PVC;Video;[GitHub](https://github.com/OpenGVLab/PVC)]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2412.09530-red?logo=arxiv" height="14" />  [Dynamic-VLM: Simple Dynamic Visual Token Compression for VideoLLM](https://arxiv.org/pdf/2412.09530). [Dynamic-VLM;Video;]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2412.04467-red?logo=arxiv" height="14" />  [VisionZip: Longer is Better but Not Necessary in Vision Language Models](https://arxiv.org/pdf/2412.04467). [VisionZip;Video;[GitHub](https://github.com/dvlab-research/VisionZip)]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2412.04449-red?logo=arxiv" height="14" />  [p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay](https://arxiv.org/pdf/2412.04449). [p-MoD: [GitHub](https://github.com/MCG-NJU/p-MoD)]

-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2412.01818-red?logo=arxiv" height="14" />  [[CLS] Attention is All You Need for Training-Free Visual Token Pruning: Make VLM Inference Faster](https://arxiv.org/pdf/2412.01818). [FasterVLM: [GitHub](https://github.com/Theia-4869/FasterVLM)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2412.00447-red?logo=arxiv" height="14" />  [ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models](https://arxiv.org/pdf/2412.00447). [ATP-LLaVA]
- OpenReview [LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token](https://openreview.net/pdf?id=UQJ7CDW8nb). [LLaVA-Mini] 
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2411.17686-red?logo=arxiv" height="14" />  [Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration](https://arxiv.org/pdf/2411.17686) .[FiCoCo;]
- OpenReview [LVP: Language-guide Visual Projector for Efficient Multimodal LLM](https://openreview.net/pdf?id=PxBzxO02Ef).[LVP]
- OpenReview [Efficient Multi-modal Large Language Models via Visual Token Grouping](https://openreview.net/pdf?id=ym1dS37mZE) .[VisToG]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2411.15024-red?logo=arxiv" height="14" />  [DyCoke:Dynamic Compression of Tokens for Fast Video Large Language Models](https://arxiv.org/pdf/2411.15024) .[DyCoke;Video;[Github](https://github.com/KD-TAO/DyCoke)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2411.14228-red?logo=arxiv" height="14" />  [FocusLLaVA: A Coarse-to-Fine Approach for Efficient and Effective Visual Token Compression](https://arxiv.org/pdf/2411.14228) .[FocusLLaVA;]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2411.10803-red?logo=arxiv" height="14" />  [MustDrop:Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large Language Model](https://arxiv.org/pdf/2411.10803) .[MustDrop;[Github](https://github.com/liuting20/MustDrop)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2411.05222-red?logo=arxiv" height="14" />  [Donâ€™t Look Twice: Faster Video Transformers with Run-Length Tokenization](https://arxiv.org/pdf/2411.05222) .[RLT;Video;NeurIPS 2024;[Github](https://rccchoudhury.github.io/projects/rlt/)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2411.03312-red?logo=arxiv" height="14" />  [Inference Optimal VLMs Need Only One Visual Token but Larger Models](https://arxiv.org/pdf/2411.03312) .[QueCC;[Github](https://github.com/locuslab/llava-token-compression)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2410.23782-red?logo=arxiv" height="14" />  [Video Token Merging for Long-form Video Understandin](https://arxiv.org/pdf/2410.23782) .[Learnable VTM;Video]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2410.17434-red?logo=arxiv" height="14" />  [LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding](https://arxiv.org/pdf/2410.17434) .[LongVU;Video;[Github](https://github.com/Vision-CAIR/LongVU)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2410.17247-red?logo=arxiv" height="14" />  [PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction](https://arxiv.org/pdf/2410.17247) .[PyramidDrop;[Github](https://github.com/Cooperx521/PyramidDrop)]

-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2410.14072-red?logo=arxiv" height="14" />  [Efficient Vision-Language Models by Summarizing Visual Tokens into Compact Registers](https://arxiv.org/pdf/2410.14072) .[Victor;]

-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2410.11417-red?logo=arxiv" height="14" />  [VidCompress: Memory-Enhanced Temporal Compression for Video Understanding in Large Language Models](https://arxiv.org/pdf/2410.11417) .[VidCompress;]

-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2410.07278-red?logo=arxiv" height="14" />  [Retrieval Replace Reduction:An effective visual token reduction method via semantic match](https://arxiv.org/pdf/2410.07278) .[TRSM;]

-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2410.02745-red?logo=arxiv" height="14" />  [AVG-LLaVA: A Large Multimodal Model with Adaptive Visual Granularity](https://arxiv.org/pdf/2410.02745) .[AVG-LLaVA;[Github](https://github.com/DeepLearnXMU/AVG-LLaVA)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2409.10994-red?logo=arxiv" height="14" />  [Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal LLMs](https://arxiv.org/pdf/2409.10994) .[TRIM]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2409.03206-red?logo=arxiv" height="14" />  [TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with Temporal Consideration](https://arxiv.org/pdf/2409.03206) .[TC-LLaVA;Video;]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2409.09564-red?logo=arxiv" height="14" />  [TG-LLaVA: Text Guided LLaVA via Learnable Latent Embeddings](https://arxiv.org/pdf/2409.09564) .[TG-LLaVA]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2409.03420-red?logo=arxiv" height="14" />  [mPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page Document Understanding](https://arxiv.org/abs/2409.03420) .    [mPLUG-DocOwl2;[Github](https://github.com/X-PLUG/mPLUG-DocOwl)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2409.01156-red?logo=arxiv" height="14" />  [TempMe: Video Temporal Token Merging for Efficient Text-Video Retrieval](https://arxiv.org/pdf/2409.01156) .    [TempMe;Video;[Github](https://github.com/X-PLUG/mPLUG-DocOwl)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2409.01179-red?logo=arxiv" height="14" />  [Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information](https://arxiv.org/pdf/2409.01179) . [Recoverable Compression]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2408.10945-red?logo=arxiv" height="14" />  [HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models in Resource-Constrained Environments](https://arxiv.org/pdf/2408.10945) . [HiRED;[Github](https://github.com/hasanar1f/HiRED)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2407.14439-red?logo=arxiv" height="14" />  [Token-level Correlation-guided Compression for Efficient Multimodal Document Understanding](https://arxiv.org/pdf/2407.14439) .    [Token-level;[Github](https://github.com/JiuTian-VL/TokenCorrCompressor)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2407.08706-red?logo=arxiv" height="14" />  [HiRes-LLaVA: Restoring Fragmentation Input in High-Resolution Large Vision-Language Models](https://arxiv.org/pdf/2407.08706) .  [HiRes-LLaVA;]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2407.02392-red?logo=arxiv" height="14" />  [TokenPacker: Efficient Visual Projector for Multimodal LLM](https://arxiv.org/abs/2407.02392.pdf) .    [TokenPacker;[Github](https://github.com/CircleRadon/TokenPacker)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2406.12275-red?logo=arxiv" height="14" />  [VoCo-LLaMA: Towards Vision Compression with Large Language Models](https://arxiv.org/pdf/2406.12275) .    [VoCo-LLaMA;[Github](https://github.com/Yxxxb/VoCo-LLaMA)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2405.20985-red?logo=arxiv" height="14" />  [DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models](https://arxiv.org/pdf/2405.20985) .    [DeCo;[Github](https://github.com/yaolinli/DeCo)]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2405.19315-red?logo=arxiv" height="14" />  [Matryoshka Query Transformer for Large Vision-Language Models](https://arxiv.org/pdf/2405.19315) .    [MQT-LLaVA; NeurIPS 2024][Github](https://github.com/gordonhu608/MQT-LLaVA)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2405.17430-red?logo=arxiv" height="14" />  [Matryoshka Multimodal Models](https://arxiv.org/pdf/2405.17430) .    [Matryoshka;M<sup>3</sup>][Github](https://github.com/mu-cai/matryoshka-mm)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2404.16821-red?logo=arxiv" height="14" />  [How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites](https://arxiv.org/abs/2404.16821.pdf) .    [InternVL;Pixel-Shuffle;[Github](https://github.com/OpenGVLab/InternVL)]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2404.08567-red?logo=arxiv" height="14" />  [CATP: Cross-Attention Token Pruning for Accuracy Preserved Multimodal Model Inference](https://arxiv.org/pdf/2404.08567) .    [CATP;]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2403.15388-red?logo=arxiv" height="14" />  [LLaVA-PruMerge:
Adaptive Token Reduction for Efficient Large Multimodal Models](https://arxiv.org/abs/2403.15388.pdf) .  [LLaVA-PruMerge;[Github](https://github.com/42Shawn/LLaVA-PruMerge)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2403.06764-red?logo=arxiv" height="14" />  [An Image is Worth 1/2 Tokens After Layer 2: Plug-and-PLay Acceleration for VLLM Inference](https://arxiv.org/pdf/2403.06764) .  [FastV;ECCV 2024;[Github](https://github.com/pkunlp-icler/FastV)]


-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2402.03766-red?logo=arxiv" height="14" />  [MobileVLM V2: Faster and Stronger Baseline for Vision Language Model](https://arxiv.org/abs/2402.03766.pdf) .    [LDP-v2;[Github](https://github.com/Meituan-AutoML/MobileVLM)]

### 2023
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2312.06742-red?logo=arxiv" height="14" /> [Honeybee: Locality-enhanced Projector for Multimodal LLM](https://arxiv.org/abs/2312.06742) . [C-Abstractor;CVPR 2024;[Github](https://github.com/khanrc/honeybee?tab=readme-ov-file) ] 
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2311.17043-red?logo=arxiv" height="14" /> [LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models](https://arxiv.org/abs/2311.17043) . [LLaMA-VID;ECCV 2024;[Github](https://github.com/dvlab-research/LLaMA-VID/tree/main) ] 
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2308.12966-red?logo=arxiv" height="14" /> [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://arxiv.org/pdf/2308.12966v2) . [Resampler;[Github](https://github.com/QwenLM/Qwen-VL)]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2305.17455-red?logo=arxiv" height="14" /> [CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers](https://arxiv.org/pdf/2305.17455v4) . [CrossGET;	ICML 2024;[Github](https://github.com/sdc17/CrossGET)]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2301.12597-red?logo=arxiv" height="14" /> [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) . [Q-former;[Github](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)]
## Vit
### 2024
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2412.00965-red?logo=arxiv" height="14" />  [Token Cropr: Faster ViTs for Quite a Few Tasks](https://arxiv.org/pdf/2412.00965).[Token Cropr;]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2408.17062-red?logo=arxiv" height="14" />  [Vote&Mix: Plug-and-Play Token Reduction for Efficient Vision Transformer](https://arxiv.org/pdf/2408.17062) . [Vote&Mix;]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2408.06798-red?logo=arxiv" height="14" />  [Token Compensator: Altering Inference Cost of Vision Transformer without Re-Tuning](https://arxiv.org/pdf/2408.06798) . [Token Compensator;ToCom;[Github](https://github.com/JieShibo/ToCom)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2408.06840-red?logo=arxiv" height="14" />  [Dynamic and Compressive Adaptation of Transformers From Images to Videos](https://arxiv.org/pdf/2408.06840) . [InTI;]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2407.12753-red?logo=arxiv" height="14" />  [LookupViT: Compressing visual information to a limited number of tokens](https://arxiv.org/pdf/2407.12753) . [LookupViT;DeepMind]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2403.09192-red?logo=arxiv" height="14" />  [PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient Task Adaptation](https://arxiv.org/abs/2403.09192) . [PYRA;ECCV 2024;[Github](https://github.com/THU-MIG/PYRA?tab=readme-ov-file)]

### 2023
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2310.01812-red?logo=arxiv" height="14" />  [PPT: Token Pruning and Pooling for Efficient Vision Transformers](https://arxiv.org/pdf/2310.01812) . [PPT;[Github](https://github.com/xjwu1024/PPT)] 
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2305.17997-red?logo=arxiv" height="14" />  [DiffRate : Differentiable Compression Rate for Efficient Vision Transformers](https://arxiv.org/abs/2305.17997) . [DiffRate;ICCV 2023;[Github](https://github.com/OpenGVLab/DiffRate)] 
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2304.10716-red?logo=arxiv" height="14" />  [Joint Token Pruning and Squeezing Towards More Aggressive Compression of Vision Transformers](https://arxiv.org/pdf/2304.10716) . [TPS;CVPR 2023;[Github](https://github.com/megvii-research/TPS-CVPR2023)]  
### 2022
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2210.09461-red?logo=arxiv" height="14" /> [TOKEN MERGING: YOUR VIT BUT FASTER](https://arxiv.org/pdf/2210.09461) . [ToMe;Token Merging; ICLR 2023]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2209.13802-red?logo=arxiv" height="14" /> [Adaptive Sparse ViT: Towards Learnable Adaptive Token Pruning by Fully Exploiting Self-Attention](https://arxiv.org/pdf/2209.13802) . [Adaptive Sparse ViT]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2202.07800-red?logo=arxiv" height="14" /> [EViT: Expediting Vision Transformers via Token Reorganizations](https://arxiv.org/pdf/2202.07800) . [EViT;ICLR 2022;[Github](https://github.com/youweiliang/evit?tab=readme-ov-file)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2201.00814-red?logo=arxiv" height="14" /> [Vision Transformer Slimming: Multi-Dimension Searching in Continuous Optimization Space](https://arxiv.org/pdf/2201.00814) . [ViT-Slim;CVPR 2022;[Github](https://github.com/Arnav0400/ViT-Slim)]
### 2021
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2112.07658-red?logo=arxiv" height="14" /> [A-ViT: Adaptive Tokens for Efficient Vision Transformer](https://arxiv.org/pdf/2112.07658) . [A-Vit;]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2111.15667-red?logo=arxiv" height="14" /> [ATS: Adaptive Token Sampling For Efficient Vision Transformers](https://arxiv.org/abs/2111.15667) . [ATS;ECCV 2022;[Github](https://github.com/adaptivetokensampling/ATS)]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2108.01390-red?logo=arxiv" height="14" /> [Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer](https://arxiv.org/abs/2108.01390) . [Evo-ViT;AAAI 2022;[Github](https://github.com/YifanXu74/Evo-ViT)]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2106.02852-red?logo=arxiv" height="14" /> [Patch Slimming for Efficient Vision Transformers](https://arxiv.org/abs/2106.02852) . [Patch Slimming;]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2106.02034-red?logo=arxiv" height="14" /> [DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsificationr](https://arxiv.org/abs/2106.02034) . [DynamicViT;NeurIPS 2021;[Github](https://github.com/raoyongming/DynamicViT)]
  



