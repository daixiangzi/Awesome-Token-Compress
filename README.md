# Awesome-Token-Compress
ðŸ”¥ðŸ”¥ðŸ”¥ A paper list of some recent  works about Token Compress for Vit and VLM.
## VLM
### 2025
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2505.20100-red?logo=arxiv" height="14" /> [AdaTP: Attention-Debiased Token Pruning for Video Large Language Models](https://arxiv.org/pdf/2505.20100). [AdaTP; Video;]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2505.17020-red?logo=arxiv" height="14" /> [CrossLMM: Decoupling Long Video Sequences from LMMs via Dual Cross-Attention Mechanisms](https://arxiv.org/pdf/2505.17020). [CrossLMM; Video; [GitHub](https://github.com/shilinyan99/CrossLMM)]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2505.15529-red?logo=arxiv" height="14" /> [Clapper: Compact Learning and Video Representation in VLMs](https://arxiv.org/pdf/2505.15529). [Clapper; Video]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2505.14454-red?logo=arxiv" height="14" /> [Video Compression Commander:Plug-and-Play Inference Acceleration for Video Large Language Models](https://arxiv.org/pdf/2505.14454). [VidCom2; Video; [GitHub](https://github.com/xuyang-liu16/VidCom2)]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2505.11945-red?logo=arxiv" height="14" />  [Top-Down Compression: Revisit Efficient Vision Token Projection for Visual Instruction Tuning](https://arxiv.org/pdf/2505.11945). [LLaVA-Meteor]
  
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2504.20384-red?logo=arxiv" height="14" />  [FiLA-Video: Spatio-Temporal Compression for Fine-Grained Long Video Understanding](https://arxiv.org/pdf/2504.20384). [FiLA-Video;Video]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2504.19627-red?logo=arxiv" height="14" />  [VCM: Vision Concept Modeling with Adaptive Vision Token Compression via Instruction Fine-Tuning](https://arxiv.org/pdf/2504.19627). [VCM]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2504.17343-red?logo=arxiv" height="14" /> [TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos](https://arxiv.org/pdf/2504.17343). [TimeChat-Online; Video; [GitHub](https://github.com/yaolinli/TimeChat-Online)]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2504.17040-red?logo=arxiv" height="14" />  [DYMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs](https://arxiv.org/pdf/2504.17040). [DYMU;[GitHub](https://github.com/MikeWangWZHL/dymu)]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2504.15270-red?logo=arxiv" height="14" />  [Quicksviewer: An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes](https://arxiv.org/pdf/2504.15270). [Quicksviewer;Video;[GitHub](https://github.com/quicksviewer/quicksviewer)]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2504.08966-red?logo=arxiv" height="14" />  [PACT: Pruning and Clustering-Based Token Reduction for Faster Visual Language Models](https://arxiv.org/pdf/2504.08966). [PACT;CVPR 2025;[GitHub](https://github.com/orailix/PACT/tree/main)]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2504.00654-red?logo=arxiv" height="14" />  [QG-VTC: Question-Guided Visual Token Compression in MLLMs for Efficient VQA](https://arxiv.org/pdf/2504.00654). [QG-VTC;VQA]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2503.21307-red?logo=arxiv" height="14" />  [InternVL-X: Advancing and Accelerating InternVL Series with Efficient Visual Token Compression](https://arxiv.org/pdf/2503.21307). [InternVL-X;[GitHub](https://github.com/ludc506/InternVL-X)]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2503.16980-red?logo=arxiv" height="14" />  [Token Dynamics: Towards Efficient and Dynamic Video Token Representation for Video Large Language Model](https://arxiv.org/pdf/2503.16980). [Token Dynamics;Video]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2503.16036-red?logo=arxiv" height="14" />  [HICom:Hybrid-Level Instruction Injection for Video Token Compression in Multi-modal Large Language Models](https://arxiv.org/pdf/2503.16036). [HICom;2025 CVPR;Video;[GitHub](https://github.com/lntzm/HICom)]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2503.11187-red?logo=arxiv" height="14" />  [FastVID: Dynamic Density Pruning for Fast Video Large Language Models](https://arxiv.org/abs/2503.11187). [FastVID;[GitHub](https://github.com/LunarShen/FastVID)]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2503.11549-red?logo=arxiv" height="14" />  [SAINTï¼šSimilarity-Aware Token Pruning: Your VLM but Faster](https://arxiv.org/pdf/2503.11549). [SAINT;[GitHub](https://github.com/ArmenJeddi/saint)]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2503.04130-red?logo=arxiv" height="14" />  [STORM:Token-Efficient Long Video Understanding for Multimodal LLMs](https://arxiv.org/pdf/2503.04130). [STORM;Video;NVIDIA]

- OpenReview [Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification](https://openreview.net/pdf?id=hzVpZDrW73). [Dynamic-LLaVA; ICLR2025; [GitHub](https://github.com/Osilly/dynamic_llava)]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2503.02175-red?logo=arxiv" height="14" />  [DivPrune: Diversity-based Visual Token Pruning for Large Multimodal Models](https://arxiv.org/pdf/2503.02175). [DivPrune;[GitHub](https://github.com/vbdi/divprune)]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2502.18512-red?logo=arxiv" height="14" />  [FCoT-VL:Advancing Text-oriented Large Vision-Language Models with Efficient Visual Token Compression](https://arxiv.org/pdf/2502.18512). [FCoT-VL;]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2501.19036-red?logo=arxiv" height="14" />  [Beyond Token Compression: A Training-Free Reduction Framework for Efficient Visual Processing in MLLMs](https://arxiv.org/pdf/2501.19036). [Beyond Token Compression;[GitHub](https://github.com/L-Hugh/Beyond-Token-Compression)]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2501.14204-red?logo=arxiv" height="14" />  [DyRate:Dynamic Token Reduction during Generation for Vision Language Models](https://arxiv.org/pdf/2501.14204). [DyRate]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2501.09532-red?logo=arxiv" height="14" />  [AdaFV: Accelerating VLMs with Self-Adaptive Cross-Modality Attention Mixture](https://arxiv.org/pdf/2501.09532). [AdaFV]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2501.03895-red?logo=arxiv" height="14" />  [LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token](https://arxiv.org/pdf/2501.03895). [LLAVA-MINI;[GitHub](https://github.com/ictnlp/LLaVA-Mini)]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2501.01986-red?logo=arxiv" height="14" />  [FrameFusion: Combining Similarity and Importance for Video Token Reduction on Large Visual Language Models](https://arxiv.org/pdf/2501.01986). [FrameFusion;Video;[GitHub](https://github.com/thu-nics/FrameFusion)]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2501.00574-red?logo=arxiv" height="14" />  [VideoChat-Flash: Hierarchical Compression for Long-Context Video Modeling](https://arxiv.org/pdf/2501.00574). [VideoChat-Flash;Video;[GitHub](https://github.com/OpenGVLab/VideoChat-Flash)]
### 2024
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2412.20504-red?logo=arxiv" height="14" />  [RETAKE: Reducing Temporal and Knowledge Redundancy for Long Video Understanding](https://arxiv.org/pdf/2412.20504). [RETAKE;Video;[GitHub](https://github.com/SCZwangxiao/video-ReTaKe)]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2412.13303-red?logo=arxiv" height="14" />  [FastVLM: Efficient Vision Encoding for Vision Language Models](https://arxiv.org/pdf/2412.13303). [FastVLM;Apple;]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2412.09613-red?logo=arxiv" height="14" />  [PVC: Progressive Visual Token Compression for Unified Image and Video Processing in Large Vision-Language Models](https://arxiv.org/pdf/2412.09613). [PVC;Video;[GitHub](https://github.com/OpenGVLab/PVC)]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2412.09530-red?logo=arxiv" height="14" />  [Dynamic-VLM: Simple Dynamic Visual Token Compression for VideoLLM](https://arxiv.org/pdf/2412.09530). [Dynamic-VLM;Video;]

- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2412.04467-red?logo=arxiv" height="14" />  [VisionZip: Longer is Better but Not Necessary in Vision Language Models](https://arxiv.org/pdf/2412.04467). [VisionZip;Video;[GitHub](https://github.com/dvlab-research/VisionZip)]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2412.04449-red?logo=arxiv" height="14" />  [p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay](https://arxiv.org/pdf/2412.04449). [p-MoD: [GitHub](https://github.com/MCG-NJU/p-MoD)]

-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2412.01818-red?logo=arxiv" height="14" />  [[CLS] Attention is All You Need for Training-Free Visual Token Pruning: Make VLM Inference Faster](https://arxiv.org/pdf/2412.01818). [FasterVLM: [GitHub](https://github.com/Theia-4869/FasterVLM)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2412.00447-red?logo=arxiv" height="14" />  [ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models](https://arxiv.org/pdf/2412.00447). [ATP-LLaVA]
- OpenReview [LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token](https://openreview.net/pdf?id=UQJ7CDW8nb). [LLaVA-Mini] 
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2411.17686-red?logo=arxiv" height="14" />  [Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration](https://arxiv.org/pdf/2411.17686) .[FiCoCo;]
- OpenReview [LVP: Language-guide Visual Projector for Efficient Multimodal LLM](https://openreview.net/pdf?id=PxBzxO02Ef).[LVP]
- OpenReview [Efficient Multi-modal Large Language Models via Visual Token Grouping](https://openreview.net/pdf?id=ym1dS37mZE) .[VisToG]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2411.15024-red?logo=arxiv" height="14" />  [DyCoke:Dynamic Compression of Tokens for Fast Video Large Language Models](https://arxiv.org/pdf/2411.15024) .[DyCoke;Video;[Github](https://github.com/KD-TAO/DyCoke)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2411.14228-red?logo=arxiv" height="14" />  [FocusLLaVA: A Coarse-to-Fine Approach for Efficient and Effective Visual Token Compression](https://arxiv.org/pdf/2411.14228) .[FocusLLaVA;]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2411.10803-red?logo=arxiv" height="14" />  [MustDrop:Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large Language Model](https://arxiv.org/pdf/2411.10803) .[MustDrop;[Github](https://github.com/liuting20/MustDrop)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2411.05222-red?logo=arxiv" height="14" />  [Don't Look Twice: Faster Video Transformers with Run-Length Tokenization](https://arxiv.org/pdf/2411.05222) .[RLT;Video;NeurIPS 2024;[Github](https://rccchoudhury.github.io/projects/rlt/)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2411.03312-red?logo=arxiv" height="14" />  [Inference Optimal VLMs Need Only One Visual Token but Larger Models](https://arxiv.org/pdf/2411.03312) .[QueCC;[Github](https://github.com/locuslab/llava-token-compression)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2410.23782-red?logo=arxiv" height="14" />  [Video Token Merging for Long-form Video Understandin](https://arxiv.org/pdf/2410.23782) .[Learnable VTM;Video]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2410.17434-red?logo=arxiv" height="14" />  [LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding](https://arxiv.org/pdf/2410.17434) .[LongVU;Video;[Github](https://github.com/Vision-CAIR/LongVU)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2410.17247-red?logo=arxiv" height="14" />  [PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction](https://arxiv.org/pdf/2410.17247) .[PyramidDrop;[Github](https://github.com/Cooperx521/PyramidDrop)]

-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2410.14072-red?logo=arxiv" height="14" />  [Efficient Vision-Language Models by Summarizing Visual Tokens into Compact Registers](https://arxiv.org/pdf/2410.14072) .[Victor;]

-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2410.11417-red?logo=arxiv" height="14" />  [VidCompress: Memory-Enhanced Temporal Compression for Video Understanding in Large Language Models](https://arxiv.org/pdf/2410.11417) .[VidCompress;]

-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2410.07278-red?logo=arxiv" height="14" />  [Retrieval Replace Reduction:An effective visual token reduction method via semantic match](https://arxiv.org/pdf/2410.07278) .[TRSM;]

-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2410.02745-red?logo=arxiv" height="14" />  [AVG-LLaVA: A Large Multimodal Model with Adaptive Visual Granularity](https://arxiv.org/pdf/2410.02745) .[AVG-LLaVA;[Github](https://github.com/DeepLearnXMU/AVG-LLaVA)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2409.10994-red?logo=arxiv" height="14" />  [Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal LLMs](https://arxiv.org/pdf/2409.10994) .[TRIM]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2409.03206-red?logo=arxiv" height="14" />  [TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with Temporal Consideration](https://arxiv.org/pdf/2409.03206) .[TC-LLaVA;Video;]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2409.09564-red?logo=arxiv" height="14" />  [TG-LLaVA: Text Guided LLaVA via Learnable Latent Embeddings](https://arxiv.org/pdf/2409.09564) .[TG-LLaVA]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2409.03420-red?logo=arxiv" height="14" />  [mPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page Document Understanding](https://arxiv.org/abs/2409.03420) .    [mPLUG-DocOwl2;[Github](https://github.com/X-PLUG/mPLUG-DocOwl)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2409.01156-red?logo=arxiv" height="14" />  [TempMe: Video Temporal Token Merging for Efficient Text-Video Retrieval](https://arxiv.org/pdf/2409.01156) .    [TempMe;Video;ICLR 2025;[Github](https://github.com/LunarShen/TempMe)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2409.01179-red?logo=arxiv" height="14" />  [Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information](https://arxiv.org/pdf/2409.01179) . [Recoverable Compression]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2408.10945-red?logo=arxiv" height="14" />  [HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models in Resource-Constrained Environments](https://arxiv.org/pdf/2408.10945) . [HiRED;[Github](https://github.com/hasanar1f/HiRED)]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2408.04840-red?logo=arxiv" height="14" />  [mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models](https://arxiv.org/abs/2408.04840). [mPLUG-Owl3;[Github](https://github.com/X-PLUG/mPLUG-Owl/tree/main/mPLUG-Owl3)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2407.14439-red?logo=arxiv" height="14" />  [Token-level Correlation-guided Compression for Efficient Multimodal Document Understanding](https://arxiv.org/pdf/2407.14439) .    [Token-level;[Github](https://github.com/JiuTian-VL/TokenCorrCompressor)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2407.08706-red?logo=arxiv" height="14" />  [HiRes-LLaVA: Restoring Fragmentation Input in High-Resolution Large Vision-Language Models](https://arxiv.org/pdf/2407.08706) .  [HiRes-LLaVA;]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2407.02392-red?logo=arxiv" height="14" />  [TokenPacker: Efficient Visual Projector for Multimodal LLM](https://arxiv.org/abs/2407.02392.pdf) .    [TokenPacker;[Github](https://github.com/CircleRadon/TokenPacker)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2406.12275-red?logo=arxiv" height="14" />  [VoCo-LLaMA: Towards Vision Compression with Large Language Models](https://arxiv.org/pdf/2406.12275) .    [VoCo-LLaMA;[Github](https://github.com/Yxxxb/VoCo-LLaMA)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2405.20985-red?logo=arxiv" height="14" />  [DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models](https://arxiv.org/pdf/2405.20985) .    [DeCo;[Github](https://github.com/yaolinli/DeCo)]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2405.19315-red?logo=arxiv" height="14" />  [Matryoshka Query Transformer for Large Vision-Language Models](https://arxiv.org/pdf/2405.19315) .    [MQT-LLaVA; NeurIPS 2024][Github](https://github.com/gordonhu608/MQT-LLaVA)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2405.17430-red?logo=arxiv" height="14" />  [Matryoshka Multimodal Models](https://arxiv.org/pdf/2405.17430) .    [Matryoshka;M<sup>3</sup>][Github](https://github.com/mu-cai/matryoshka-mm)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2404.16821-red?logo=arxiv" height="14" />  [How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites](https://arxiv.org/abs/2404.16821.pdf) .    [InternVL;Pixel-Shuffle;[Github](https://github.com/OpenGVLab/InternVL)]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2404.08567-red?logo=arxiv" height="14" />  [CATP: Cross-Attention Token Pruning for Accuracy Preserved Multimodal Model Inference](https://arxiv.org/pdf/2404.08567) .    [CATP;]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2403.15388-red?logo=arxiv" height="14" />  [LLaVA-PruMerge:
Adaptive Token Reduction for Efficient Large Multimodal Models](https://arxiv.org/abs/2403.15388.pdf) .  [LLaVA-PruMerge;[Github](https://github.com/42Shawn/LLaVA-PruMerge)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2403.06764-red?logo=arxiv" height="14" />  [An Image is Worth 1/2 Tokens After Layer 2: Plug-and-PLay Acceleration for VLLM Inference](https://arxiv.org/pdf/2403.06764) .  [FastV;ECCV 2024;[Github](https://github.com/pkunlp-icler/FastV)]


-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2402.03766-red?logo=arxiv" height="14" />  [MobileVLM V2: Faster and Stronger Baseline for Vision Language Model](https://arxiv.org/abs/2402.03766.pdf) .    [LDP-v2;[Github](https://github.com/Meituan-AutoML/MobileVLM)]

### 2023
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2312.06742-red?logo=arxiv" height="14" /> [Honeybee: Locality-enhanced Projector for Multimodal LLM](https://arxiv.org/abs/2312.06742) . [C-Abstractor;CVPR 2024;[Github](https://github.com/khanrc/honeybee?tab=readme-ov-file) ] 
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2311.17043-red?logo=arxiv" height="14" /> [LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models](https://arxiv.org/abs/2311.17043) . [LLaMA-VID;ECCV 2024;[Github](https://github.com/dvlab-research/LLaMA-VID/tree/main) ] 
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2308.12966-red?logo=arxiv" height="14" /> [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://arxiv.org/pdf/2308.12966v2) . [Resampler;[Github](https://github.com/QwenLM/Qwen-VL)]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2305.17455-red?logo=arxiv" height="14" /> [CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers](https://arxiv.org/pdf/2305.17455v4) . [CrossGET;	ICML 2024;[Github](https://github.com/sdc17/CrossGET)]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2301.12597-red?logo=arxiv" height="14" /> [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) . [Q-former;[Github](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)]
## Vit
### 2025
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2505.15160-red?logo=arxiv" height="14" />  [Lossless Token Merging Even Without Fine-Tuning in Vision Transformers](https://arxiv.org/pdf/2505.15160) . [ATM]

-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2503.23455-red?logo=arxiv" height="14" />  [Prune and Merge: Efficient Token Compression For Vision Transformer With Spatial Information Preserved](https://arxiv.org/pdf/2503.23455) . [Prune and Merge;[Github](https://github.com/NUST-Machine-Intelligence-Laboratory/prune_and_merge)]

### 2024
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2412.00965-red?logo=arxiv" height="14" />  [Token Cropr: Faster ViTs for Quite a Few Tasks](https://arxiv.org/pdf/2412.00965).[Token Cropr;]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2408.17062-red?logo=arxiv" height="14" />  [Vote&Mix: Plug-and-Play Token Reduction for Efficient Vision Transformer](https://arxiv.org/pdf/2408.17062) . [Vote&Mix;]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2408.06798-red?logo=arxiv" height="14" />  [Token Compensator: Altering Inference Cost of Vision Transformer without Re-Tuning](https://arxiv.org/pdf/2408.06798) . [Token Compensator;ToCom;[Github](https://github.com/JieShibo/ToCom)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2408.06840-red?logo=arxiv" height="14" />  [Dynamic and Compressive Adaptation of Transformers From Images to Videos](https://arxiv.org/pdf/2408.06840) . [InTI;]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2407.12753-red?logo=arxiv" height="14" />  [LookupViT: Compressing visual information to a limited number of tokens](https://arxiv.org/pdf/2407.12753) . [LookupViT;DeepMind]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2403.09192-red?logo=arxiv" height="14" />  [PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient Task Adaptation](https://arxiv.org/abs/2403.09192) . [PYRA;ECCV 2024;[Github](https://github.com/THU-MIG/PYRA?tab=readme-ov-file)]

### 2023
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2310.01812-red?logo=arxiv" height="14" />  [PPT: Token Pruning and Pooling for Efficient Vision Transformers](https://arxiv.org/pdf/2310.01812) . [PPT;[Github](https://github.com/xjwu1024/PPT)] 
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2305.17997-red?logo=arxiv" height="14" />  [DiffRate : Differentiable Compression Rate for Efficient Vision Transformers](https://arxiv.org/abs/2305.17997) . [DiffRate;ICCV 2023;[Github](https://github.com/OpenGVLab/DiffRate)] 
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2304.10716-red?logo=arxiv" height="14" />  [Joint Token Pruning and Squeezing Towards More Aggressive Compression of Vision Transformers](https://arxiv.org/pdf/2304.10716) . [TPS;CVPR 2023;[Github](https://github.com/megvii-research/TPS-CVPR2023)]  
### 2022
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2210.09461-red?logo=arxiv" height="14" /> [TOKEN MERGING: YOUR VIT BUT FASTER](https://arxiv.org/pdf/2210.09461) . [ToMe;Token Merging; ICLR 2023]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2209.13802-red?logo=arxiv" height="14" /> [Adaptive Sparse ViT: Towards Learnable Adaptive Token Pruning by Fully Exploiting Self-Attention](https://arxiv.org/pdf/2209.13802) . [Adaptive Sparse ViT]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2202.07800-red?logo=arxiv" height="14" /> [EViT: Expediting Vision Transformers via Token Reorganizations](https://arxiv.org/pdf/2202.07800) . [EViT;ICLR 2022;[Github](https://github.com/youweiliang/evit?tab=readme-ov-file)]
-  <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2201.00814-red?logo=arxiv" height="14" /> [Vision Transformer Slimming: Multi-Dimension Searching in Continuous Optimization Space](https://arxiv.org/pdf/2201.00814) . [ViT-Slim;CVPR 2022;[Github](https://github.com/Arnav0400/ViT-Slim)]
### 2021
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2112.07658-red?logo=arxiv" height="14" /> [A-ViT: Adaptive Tokens for Efficient Vision Transformer](https://arxiv.org/pdf/2112.07658) . [A-Vit;]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2111.15667-red?logo=arxiv" height="14" /> [ATS: Adaptive Token Sampling For Efficient Vision Transformers](https://arxiv.org/abs/2111.15667) . [ATS;ECCV 2022;[Github](https://github.com/adaptivetokensampling/ATS)]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2108.01390-red?logo=arxiv" height="14" /> [Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer](https://arxiv.org/abs/2108.01390) . [Evo-ViT;AAAI 2022;[Github](https://github.com/YifanXu74/Evo-ViT)]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2106.02852-red?logo=arxiv" height="14" /> [Patch Slimming for Efficient Vision Transformers](https://arxiv.org/abs/2106.02852) . [Patch Slimming;]
- <img alt="arXiv" src="https://img.shields.io/badge/arXiv-2106.02034-red?logo=arxiv" height="14" /> [DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsificationr](https://arxiv.org/abs/2106.02034) . [DynamicViT;NeurIPS 2021;[Github](https://github.com/raoyongming/DynamicViT)]
  



